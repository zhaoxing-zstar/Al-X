{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "前馈神经网络.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCr53LZRlFpa"
      },
      "source": [
        "### 首先导入数据\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvjUFACqnFqb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class NumpyOp:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.memory = {}\n",
        "        self.epsilon = 1e-12\n",
        "\n",
        "\n",
        "class Matmul(NumpyOp):\n",
        "    \n",
        "    def forward(self, x, W):\n",
        "        \"\"\"\n",
        "        x: shape(N, d)\n",
        "        w: shape(d, d')\n",
        "        \"\"\"\n",
        "        self.memory['x'] = x\n",
        "        self.memory['W'] = W\n",
        "        h = np.matmul(x, W)\n",
        "        return h\n",
        "    \n",
        "    def backward(self, grad_y):\n",
        "        \"\"\"\n",
        "        grad_y: shape(N, d')\n",
        "        \"\"\"\n",
        "        \n",
        "        x, W= self.memory['x'], self.memory['W']\n",
        "        grad_x=np.matmul(grad_y, W.T)\n",
        "        grad_W=np.matmul(x.T, grad_y)\n",
        "        \n",
        "        return grad_x, grad_W\n",
        "\n",
        "\n",
        "class Relu(NumpyOp):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.memory['x'] = x\n",
        "        return np.where(x > 0, x, np.zeros_like(x))\n",
        "    \n",
        "    def backward(self, grad_y):\n",
        "        \"\"\"\n",
        "        grad_y: same shape as x\n",
        "        \"\"\"\n",
        "        x=self.memory['x']\n",
        "        grad_x= grad_y*np.where(x>0, np.ones_like(x), np.zeros_like(x))\n",
        "        \n",
        "        return grad_x\n",
        "\n",
        "\n",
        "class Log(NumpyOp):\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: shape(N, c)\n",
        "        \"\"\"\n",
        "  \n",
        "        out = np.log(x + self.epsilon)\n",
        "        self.memory['x'] = x\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, grad_y):\n",
        "        \"\"\"\n",
        "        grad_y: same shape as x\n",
        "        \"\"\"\n",
        "        x=self.memory['x']\n",
        "        grad_x=grad_y* np.reciprocal(x+self.epsilon)\n",
        "        \n",
        "        return grad_x\n",
        "\n",
        "\n",
        "class Softmax(NumpyOp):\n",
        "    \"\"\"\n",
        "    softmax over last dimension\n",
        "    \"\"\"\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: shape(N, c)\n",
        "        \"\"\"\n",
        "        self.memory['x']=x\n",
        "        exp_x=np.exp(x-np.max(x,axis=1,keepdims=True))\n",
        "        out=exp_x/np.sum(exp,axis=1,keepdims=True)\n",
        "        self.memory['out']=out\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, grad_y):\n",
        "        \"\"\"\n",
        "        grad_y: same shape as x\n",
        "        \"\"\"\n",
        "        y=self.memory['out']\n",
        "        Jaco = np.array([np.diag(r)-np.outer(r,r) for r in y])\n",
        "\n",
        "        grad_y=grad_y[:, np.newaxis, : ]\n",
        "        grad_x=np.matmul(grad_y, Jaco).squeeze(axis=1)\n",
        "\n",
        "        return grad_x\n",
        "\n",
        "\n",
        "class NumpyLoss:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.target = None\n",
        "    \n",
        "    def get_loss(self, pred, target):\n",
        "        self.target = target\n",
        "        return (-pred * target).sum(axis=1).mean()\n",
        "    \n",
        "    def backward(self):\n",
        "        return -self.target / self.target.shape[0]\n",
        "\n",
        "\n",
        "class NumpyModel:\n",
        "    def __init__(self):\n",
        "        self.W1 = np.random.normal(size=(28 * 28, 256))\n",
        "        self.W2 = np.random.normal(size=(256, 64))\n",
        "        self.W3 = np.random.normal(size=(64, 10))\n",
        "        \n",
        "        # 以下算子会在 forward 和 backward 中使用\n",
        "        self.matmul_1 = Matmul()\n",
        "        self.relu_1 = Relu()\n",
        "        self.matmul_2 = Matmul()\n",
        "        self.relu_2 = Relu()\n",
        "        self.matmul_3 = Matmul()\n",
        "        self.softmax = Softmax()\n",
        "        self.log = Log()\n",
        "        \n",
        "        # 以下变量需要在 backward 中更新。 softmax_grad, log_grad 等为算子反向传播的梯度（ loss 关于算子输入的偏导）\n",
        "        self.x1_grad, self.W1_grad = None, None\n",
        "        self.relu_1_grad = None\n",
        "        self.x2_grad, self.W2_grad = None, None\n",
        "        self.relu_2_grad = None\n",
        "        self.x3_grad, self.W3_grad = None, None\n",
        "        self.softmax_grad = None\n",
        "        self.log_grad = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 28 * 28)\n",
        "        \n",
        "        z1 = self.matmul_1.forward(x, self.W1)\n",
        "        x2 = self.relu_1.forward(z1)\n",
        "        z2 = self.matmul_2.forward(x2, self.W2)\n",
        "        x3 = self.relu_2.forward(z2)\n",
        "        z3 = self.matmul_3.forward(x3, self.W3)\n",
        "        out = self.softmax.forward(z3)\n",
        "        x = self.log.forward(out)\n",
        "        \n",
        "\n",
        "        return x\n",
        "    \n",
        "    def backward(self, y):\n",
        "        self.log_grad = self.log.backward(y)   \n",
        "        self.softmax_grad = self.softmax.backward(self.log_grad)\n",
        "        self.x3_grad, self.W3_grad = self.matmul_3.backward(self.softmax_grad)\n",
        "        self.relu_2_grad = self.relu_2.backward(self.x3_grad)\n",
        "        self.x2_grad, self.W2_grad = self.matmul_2.backward(self.relu_2_grad)\n",
        "        self.relu_1_grad = self.relu_1.backward(self.x2_grad)\n",
        "        self.x1_grad, self.W1_grad = self.matmul_1.backward(self.relu_1_grad)\n",
        "        \n",
        "    \n",
        "    def optimize(self, learning_rate):\n",
        "        self.W1 -= learning_rate * self.W1_grad\n",
        "        self.W2 -= learning_rate * self.W2_grad\n",
        "        self.W3 -= learning_rate * self.W3_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkXAysWG0h5L"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def plot_curve(data):\n",
        "    plt.plot(range(len(data)), data, color='blue')\n",
        "    plt.legend(['loss_value'], loc='upper right')\n",
        "    plt.xlabel('step')\n",
        "    plt.ylabel('value')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def download_mnist():\n",
        "    from torchvision import datasets, transforms\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "    ])\n",
        "    \n",
        "    train_dataset = datasets.MNIST(root=\"./data/\", transform=transform, train=True, download=True)\n",
        "    test_dataset = datasets.MNIST(root=\"./data/\", transform=transform, train=False, download=True)\n",
        "    \n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def one_hot(y, numpy=True):\n",
        "    if numpy:\n",
        "        y_ = np.zeros((y.shape[0], 10))\n",
        "        y_[np.arange(y.shape[0], dtype=np.int32), y] = 1\n",
        "        return y_\n",
        "    else:\n",
        "        y_ = torch.zeros((y.shape[0], 10))\n",
        "        y_[torch.arange(y.shape[0], dtype=torch.long), y] = 1\n",
        "    return y_\n",
        "\n",
        "\n",
        "def batch(dataset, numpy=True):\n",
        "    data = []\n",
        "    label = []\n",
        "    for each in dataset:\n",
        "        data.append(each[0])\n",
        "        label.append(each[1])\n",
        "    data = torch.stack(data)\n",
        "    label = torch.LongTensor(label)\n",
        "    if numpy:\n",
        "        return [(data.numpy(), label.numpy())]\n",
        "    else:\n",
        "        return [(data, label)]\n",
        "\n",
        "\n",
        "def mini_batch(dataset, batch_size=128, numpy=False):\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "def get_torch_initialization(numpy=True):\n",
        "    fc1 = torch.nn.Linear(28 * 28, 256)\n",
        "    fc2 = torch.nn.Linear(256, 64)\n",
        "    fc3 = torch.nn.Linear(64, 10)\n",
        "    \n",
        "    if numpy:\n",
        "        W1 = fc1.weight.T.detach().clone().numpy()\n",
        "        W2 = fc2.weight.T.detach().clone().numpy()\n",
        "        W3 = fc3.weight.T.detach().clone().numpy()\n",
        "    else:\n",
        "        W1 = fc1.weight.T.detach().clone().data\n",
        "        W2 = fc2.weight.T.detach().clone().data\n",
        "        W3 = fc3.weight.T.detach().clone().data\n",
        "    \n",
        "    return W1, W2, W3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ao5ZSzQT0O8s",
        "outputId": "f1c42663-61a7-4d68-c730-52ee6daa0e78"
      },
      "source": [
        "import torch\n",
        "# from utils import mini_batch, batch, download_mnist, get_torch_initialization, one_hot, plot_curve\n",
        "\n",
        "\n",
        "class TorchModel:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.W1 = torch.randn((28 * 28, 256), requires_grad=True)\n",
        "        self.W2 = torch.randn((256, 64), requires_grad=True)\n",
        "        self.W3 = torch.randn((64, 10), requires_grad=True)\n",
        "        self.softmax_input = None\n",
        "        self.log_input = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 28 * 28)\n",
        "        x = torch.relu(torch.matmul(x, self.W1))\n",
        "        x = torch.relu(torch.matmul(x, self.W2))\n",
        "        x = torch.matmul(x, self.W3)\n",
        "        \n",
        "        self.softmax_input = x\n",
        "        self.softmax_input.retain_grad()\n",
        "        \n",
        "        x = torch.softmax(x, 1)\n",
        "        \n",
        "        self.log_input = x\n",
        "        self.log_input.retain_grad()\n",
        "        \n",
        "        x = torch.log(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def optimize(self, learning_rate):\n",
        "        with torch.no_grad():\n",
        "            self.W1 -= learning_rate * self.W1.grad\n",
        "            self.W2 -= learning_rate * self.W2.grad\n",
        "            self.W3 -= learning_rate * self.W3.grad\n",
        "            \n",
        "            self.W1.grad = None\n",
        "            self.W2.grad = None\n",
        "            self.W3.grad = None\n",
        "\n",
        "\n",
        "def torch_run():\n",
        "    train_dataset, test_dataset = download_mnist()\n",
        "    \n",
        "    model = TorchModel()\n",
        "    model.W1.data, model.W2.data, model.W3.data = get_torch_initialization(numpy=False)\n",
        "    \n",
        "    train_loss = []\n",
        "    \n",
        "    epoch_number = 10\n",
        "    learning_rate = 0.1\n",
        "    \n",
        "    for epoch in range(epoch_number):\n",
        "        for x, y in mini_batch(train_dataset, numpy=False):\n",
        "            y = one_hot(y, numpy=False)\n",
        "            \n",
        "            y_pred = model.forward(x)\n",
        "            loss = (-y_pred * y).sum(dim=1).mean()\n",
        "            loss.backward()\n",
        "            model.optimize(learning_rate)\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "        \n",
        "        x, y = batch(test_dataset, numpy=False)[0]\n",
        "        accuracy = model.forward(x).argmax(dim=1).eq(y).float().mean().item()\n",
        "        print('[{}] Accuracy: {:.4f}'.format(epoch, accuracy))\n",
        "    \n",
        "    plot_curve(train_loss)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch_run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] Accuracy: 0.9406\n",
            "[1] Accuracy: 0.9651\n",
            "[2] Accuracy: 0.9710\n",
            "[3] Accuracy: 0.9647\n",
            "[4] Accuracy: 0.9755\n",
            "[5] Accuracy: 0.9766\n",
            "[6] Accuracy: 0.9790\n",
            "[7] Accuracy: 0.9762\n",
            "[8] Accuracy: 0.9779\n",
            "[9] Accuracy: 0.9793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgU1b3/8ffXAUHFBYHggrIYXBNEg141iRITFE3UeDXB5dG44b1RiWiij+aK3ms0Ji43RuSKJjHKT2NG0UQ0EIUE16g4IAi4sLiCGLawyTbDfH9/nGq7e7p7pmepqRnq83qeerqW01Wna2b6M1Wn6pS5OyIikl7bJF0BERFJloJARCTlFAQiIimnIBARSTkFgYhIynVIugKN1b17d+/Tp0/S1RARaVemT5++3N17FFvW7oKgT58+VFVVJV0NEZF2xcw+LLVMp4ZERFJOQSAiknIKAhGRlGt3bQQi0v5VV1ezaNEiNm7cmHRVtjqdO3emV69edOzYsez3KAhEpNUtWrSIHXfckT59+mBmSVdnq+HurFixgkWLFtG3b9+y36dTQyLS6jZu3Ei3bt0UAi3MzOjWrVujj7QUBCKSCIVAPJqyX1MTBHPmwKhRsGxZ0jUREWlbUhME77wDN90En36adE1ERNqW1ARBp07hddOmZOshIm1Dly5dkq5CnvPOO4/x48cnsm0FgYhIyqXm8lEFgUjbNHIkzJzZsuscOBDuvLO8su7O1VdfzaRJkzAzrrvuOoYNG8aSJUsYNmwYa9asoaamhnvuuYejjjqKCy+8kKqqKsyMCy64gCuuuKJgne+88w7nnnsu06ZNA+CDDz7gpJNOYvbs2dx444089dRTbNiwgaOOOop77723oIE306da9+7dqaqq4ic/+QnPPfccn332GSNGjGDOnDlUV1fz3//935xyyinN3l86IhCRVHviiSeYOXMms2bNYsqUKVx11VUsWbKEP/zhDxx//PGfLxs4cCAzZ85k8eLFzJkzh9mzZ3P++ecXXef+++/P5s2bef/99wGorKxk2LBhAFx22WW8/vrrzJkzhw0bNvD000+XXdebb76ZY489lmnTpjF16lSuuuoqPvvss2bvAx0RiEiiyv3PPS4vvfQSZ555JhUVFfTs2ZNjjjmG119/ncMOO4wLLriA6upqvvvd7zJw4ED69evHe++9x4gRI/j2t7/NcccdV3K93//+96msrOSaa66hsrKSyspKAKZOncqtt97K+vXrWblyJQcddBAnnXRSWXV99tlnmTBhArfffjsQ7sf46KOPOOCAA5q1D3REICJSxNFHH80LL7zAnnvuyXnnnce4cePo2rUrs2bNYvDgwYwdO5aLLrqo5PuHDRvGo48+yrx58zAz+vfvz8aNG7nkkksYP348s2fPZvjw4UVv/urQoQO1tbUAecvdnccff5yZM2cyc+bMFgkBSFEQbLtteFUQiEiur3/961RWVrJlyxaWLVvGCy+8wOGHH86HH35Iz549GT58OBdddBEzZsxg+fLl1NbWctppp3HTTTcxY8aMkuvdZ599qKio4Gc/+9nnp4UyX+rdu3dn3bp1Ja8S6tOnD9OnTwfg8ccf/3z+8ccfz+jRo3F3AN54440W2Qc6NSQiqXbqqafyyiuvcPDBB2Nm3Hrrrey22248+OCD3HbbbXTs2JEuXbowbtw4Fi9ezPnnn//5f+u33HJLveseNmwYV1111edtBbvssgvDhw/nS1/6ErvtthuHHXZY0ffdcMMNXHjhhYwaNYrBgwd/Pn/UqFGMHDmSAQMGUFtbS9++fRvVxlCKZZKlvRg0aJA35QllS5dCz55w991w6aUxVExEyvb222+3yCkNKa7Y/jWz6e4+qFj51Jwa0hGBiEhxOjUkItIMl156KS+//HLevMsvv7zkpaVtUWqCINNYvHlzsvUQkcDdt4oeSMeMGZN0FfI05XR/ak4NbRN90i1bkq2HiISnaK1YsaJJX1pSWubBNJ07d27U+1JzRABQUaEgEGkLevXqxaJFi1imfuFbXOZRlY2hIBCRVtexY8dGPUpR4pWaU0OgIBARKUZBICKScgoCEZGUUxCIiKScgkBEJOUUBCIiKacgEBFJudiCwMz2MrOpZvaWmc01s8uLlDEzu8vMFpjZm2Z2aFz1AQWBiEgxcd5QVgP82N1nmNmOwHQzm+zub+WUOQHoHw3/BtwTvcZCQSAiUii2IwJ3X+LuM6LxtcDbwJ51ip0CjPPgVWAXM9s9rjopCERECrVKG4GZ9QEOAV6rs2hP4OOc6UUUhgVmdrGZVZlZVXP6JlEQiIgUij0IzKwL8Dgw0t3XNGUd7n6fuw9y90E9evRocl0UBCIihWINAjPrSAiBh939iSJFFgN75Uz3iubFQkEgIlIozquGDPgd8La7/2+JYhOAc6Orh44AVrv7krjqpCAQESkU51VDXwXOAWab2cxo3k+BvQHcfSwwETgRWACsB2J9tpuCQESkUGxB4O4vAfU+h87D44kujasOdSkIREQK6c5iEZGUUxCIiKScgkBEJOUUBCIiKacgEBFJOQWBiEjKKQhERFJOQSAiknIKAhGRlFMQiIiknIJARCTlFAQiIimnIBARSTkFgYhIyikIRERSTkEgIpJyCgIRkZRLVRAArFyZdA1ERNqWVAXBPfdAbW0YREQkSFUQDBkSXhUEIiJZqQqCwYPDq4JARCQrVUGwTfRpFQQiIlkKAhGRlFMQiIiknIJARCTlFAQiIimnIBARSTkFgYhIyikIRERSTkEgIpJyCgIRkZRTEIiIpJyCQEQk5RQEIiIpF1sQmNn9ZrbUzOaUWD7YzFab2cxouD6uumQoCERECnWIcd0PAHcD4+op86K7fyfGOuRREIiIFIrtiMDdXwDa1IMhFQQiIoWSbiM40sxmmdkkMzuoVCEzu9jMqsysatmyZU3emIJARKRQkkEwA+jt7gcDo4E/lyro7ve5+yB3H9SjR48mb1BBICJSKLEgcPc17r4uGp8IdDSz7nFuU0EgIlIosSAws93MzKLxw6O6rIhzmwoCEZFCsV01ZGaPAIOB7ma2CLgB6Ajg7mOB04EfmlkNsAE4w909rvqAgkBEpJjYgsDdz2xg+d2Ey0tbTSYI4o0bEZH2JemrhlqVjghERAopCEREUk5BICKScgoCEZGUUxCIiKScgkBEJOUUBCIiKacgEBFJOQWBiEjKpSoIttsuvK5dm2w9RETaklQFwQ47hNcNG5Kth4hIW5KqIOgQ9ay0ZUuy9RARaUtSFQQVFeG1pibZeoiItCWpCgIdEYiIFGowCMysp5n9zswmRdMHmtmF8Vet5emIQESkUDlHBA8AzwB7RNPzgJFxVShOmSMCBYGISFY5QdDd3R8FagHcvQZolydXdGpIRKRQOUHwmZl1AxzAzI4AVsdaq5jo1JCISKFyHlV5JTAB2MfMXgZ6EJ433O7oiEBEpFCDQeDuM8zsGGA/wIB33b069prFQEcEIiKFGgwCMzu3zqxDzQx3HxdTnWKjIwIRkULlnBo6LGe8M/BNYAbQ7oJARwQiIoXKOTU0InfazHYB/hhbjWJkFnogVRCIiGQ15c7iz4C+LV2R1tKhg4JARCRXOW0ETxFdOkoIjgOBR+OsVJwqKtRGICKSq5w2gttzxmuAD919UUz1iZ2CQEQkXzltBM+3RkVai4JARCRfySAws7VkTwnlLQLc3XeKrVYxUhCIiOQrGQTuvmNrVqS1KAhERPKV00YAgJl9gXAfAQDu/lEsNYqZgkBEJF85zyM42czmA+8DzwMfAJNirldsFAQiIvnKuY/gZ8ARwDx370u4s/jVWGsVIwWBiEi+coKg2t1XANuY2TbuPhUYFHO9YqMgEBHJV04bwSoz6wK8CDxsZksJdxe3S7qzWEQkXzlHBFOBnYHLgb8CC4GTGnqTmd1vZkvNbE6J5WZmd5nZAjN708wObUzFm0pHBCIi+coJgg7As8BzwI5AZXSqqCEPAEPrWX4C0D8aLgbuKWOdzaYgEBHJ12AQuPv/uPtBwKXA7sDzZjaljPe9AKysp8gpwDgPXgV2MbPdy6x3kykIRETyNab30aXAp8AK4AstsO09gY9zphdF8wqY2cVmVmVmVcuWLWvWRhUEIiL5yrmP4BIzew74G9ANGO7uA+KuWC53v8/dB7n7oB49ejRrXQoCEZF85Vw1tBcw0t1ntvC2F0frzugVzYuVgkBEJF85bQTXxhACABOAc6Orh44AVrv7khi2k0dBICKSr+y+hhrLzB4BBgPdzWwRcAPQEcDdxwITgROBBcB64Py46pJLQSAiki+2IHD3MxtY7oQrkVpVhw5QXd3aWxURabua8szidq26Gl56KelaiIi0HakLgkwIzJ2bbD1ERNqK1AVBxqpVSddARKRtSG0QiIhIkNog+Mtfkq6BiEjbkNoguOWWpGsgItI2pDYIREQkUBCIiKScgkBEJOUUBCIiKacgEBFJOQWBiEjKKQhERFJOQSAiknIKAhGRlFMQiIikXKqDYO3apGsgIpK8VAfBhx8mXQMRkeSlOgi2SfWnFxEJUv1VuGFD0jUQEUleqoNg0CB4+OGkayEikqzUBUG3bvnTekCNiKRd6oKgoiLpGoiItC2pC4Kzzkq6BiIibUvqguD225OugYhI25K6IKh7amjyZFi2LJm6iIi0BakLgrqWL4dvfjPpWoiIJCf1QQAwd27SNRARSY6CAHBPugYiIslREKAgEJF0UxCIiKRcKoNg4sTCeatXwyuvtH5dRESSlsog2GGHwnknnwxHHQUbN7Z+fUREkpTKIOjQoXDetGnhtba2desiIpK0WIPAzIaa2btmtsDMrimy/DwzW2ZmM6Phojjrk1Gsv6FMg7FZa9RARKTtKPK/ccswswpgDDAEWAS8bmYT3P2tOkUr3f2yuOpRTH1B8Omn0Ldva9ZGRCRZcR4RHA4scPf33H0z8EfglBi3V7b6gqBfv9ati4hI0uIMgj2Bj3OmF0Xz6jrNzN40s/FmtlexFZnZxWZWZWZVy1qgY6Bij6isrs6OT5kC77zT7M2IiLQLSTcWPwX0cfcBwGTgwWKF3P0+dx/k7oN69OjR7I029KziIUPggAPUcCwi6RBnECwGcv/D7xXN+5y7r3D3TdHkb4GvxFifz5XbIDxmTLz1EBFpC+IMgteB/mbW18y2Bc4AJuQWMLPdcyZPBt6OsT6N9tFHhfN+9jP4059avy4iInGJ7aohd68xs8uAZ4AK4H53n2tmNwJV7j4B+JGZnQzUACuB8+KqT37dyiu3alW42/jII7Pzrr++cesQEWnrzNvZN9qgQYO8qqqqWeuYPRsGDCi//MaN0KlTGM+cVoprt915J9x6K3zySTzrF5F0MrPp7j6o2LKkG4sT0bFj48o/9FA89SjmiitgyZLW256ISCqDYL/9Glf+ootg++3h8cfjqY+ISJJSGQRN6UZiwwY466zGv++XvwwNzCIibVUq2wig+X0KlbvbGtumEHcbhIikk9oIivi//0u6BiIibUNqg+A//zPpGtRPRwQi0lpSGwTNPTX0u9/B3//eMnUpRkEgIq0lthvKtnYXRU9O+OyzcEXRggXQu3d46E0mZFavbvr6a2sb7hNJRKQl6KummXbYAZ59Fvr3h223hXvvDfM3boRddmn6etXhnYi0llQHwejRLbOezGMuIdwVDHD22cXL1tRkv+QXL4Z//at4OZ0aEpHWktrLRzPieDTlzjsXnhbKfRTmCSfAxIlhfKed8stm6pM55VSfwYNh/vwQKCIi9dHlo62sobaBSZOy42vWFC9TTj4//3zL90m0ZQuccQa88UbLrldE2i4FQUJyn4A2fDhcemn+8szpo9pa+OMfW6/NYMECqKwMYSAi6aAgaCXuMGtWdvqAA7Ljv/1t4Q1uf/sbbN4M3/senHlmthE6Y8uW+OoqIumiIGglDz8MAweWX/7UU+GnP4UnngjT//xn/vJrry18z667wrBhTa9jrnbWdCQizaAgqOOQQ+JZ7znnNFzm7TrPZ3v33ey4GUyfHhqily6FZ57JLvvVr+C558IVSI8+2rx6xtF4LiJtm24oq+P3v2/cf+4tafr00svM4PbbQ+PylCn5/7FfeWXD6964Mawj84CdumbOhG7dstPz55dXZxFp/1J/RDB6NNx9N3zpS2E6ybt5R43Kn3766ez4DTeERmMIDcf1nbr593+H5cvz5223HXTunF1HXYccAnvvnT+v2DObM2bMCMFS9yhGRNqf1AfBZZeFK3Yyjbc77phcXT74oLxy55xTfxD86U/hkZfFnHlm4X/7pU4n9e5duk6PPZbdVsaLL6pXV5H2KPVBkHH//eGUS58+SdekPHPn1r+8ogI+/BCGDoW1a/OX5QbB6tX5Dcx12whOP73wlNWyZdlTTJs2ZecffXThZbCydVq/HtatS7oW0lIUBJEuXeCb30y6Fi3nySdDqD3zDPz5z/nLco8mPvyw/vVMnw6DonsRFy8OQfGFL8A//pEtM3cuzJ6dnc69Ya6xxowJ26iuzl9/3TB75ZVwz4MkY9ddkz16lpalINhK5d6zUPe//FdegcmTwxf2wQeXt74f/xhuvjk7PXlyeK2tDe0rAwZkl514YtO7vbj++vC6Zk2op1lY/wkn5Jc76qjQ0d8//hHaeACuuSaUb+s3w519NtxxR9K1aJ7cI0HZCrh7uxq+8pWveNwuv9x9n33c33nHPfz/7H7qqdnx9jZ06FB+2XnzGrfukSNLr6eYGTPc160L46ed5r7bbvnLu3cP799nn8J1VlVly9Vd9tJL+dOt7a9/dd9mG/d//avhsknVsSVtDZ+hvdiyJezrX/+6eesBqrzE92riX+yNHVojCHJlfuE3bEj+C701hsYGQadOxedfe23+fqytdV+0KCw78cT8fZurZ8/6tzdrVv57Sw0Neest98mTm//7kXHUUWG7L77YcNmt4Uu0LX2Gf/wj1OWdd5KuSTw2bw6fr0OH5q2nviDQqaEyde4MK1eWXn7jja1Xlzjtu2/jypc6RTB3briE9bTTwn77xS+gV6+wbOLE/LK57Q0NXb776aewYkXD9Vq/Hh56KHxdFXPggTBkSPFl778PS5Y0vI1cmb6g6qv/7Nn137C3cWPodRZg1ary2kBqauA3v9n6uhypeyd9ff7wh/Cae5Pl1qTU73BLUhA0Qteu2S4f6ho6tHXr0tZNmAA9eoT91a1b6C4j16uvZse/+tXseENfwGbQvXvD2x85Mlxm++KL2XnLlxe/UW76dFi4MDRQr1wJ/frBHnvkl9m0Cd57r/S9FZkgqKgIV5+9+WZhmbqN6HXX1bt3uGgBQgN9//6lP1/GXXfBxRcX9kV1221wySWl37d4MZx8cmEjfFswdSrstlv+pclppiBoAw45JP+L6tRTi/9g9tsvvH7rW9nHWEppRx6ZP921a/13VjfWokXhNfNF5x6Cqe4Rz69/Hb50v/hFuOCC/Lurc3XuDPvsE76sAT7+GDp2zDbK5x4RDBlSvBG+7u9N795QVZXtinzp0uyyhQuL12PDhnBz4DHHhPFMsNV9wNHVV8M995TutfaGG+Cpp/JvMFy6NITn+vXF39NUK1dmj6bLaWTOPG4k90gxzRQEbcCMGfDSS4XzO3cOr5mzpTvtFF4nT9a19E2xalX2MtX6vPxyeevLfAF+5zvhKCL3lM2UKdnxkSOz4w8/nL+OzNHE5s3586++OtyFXVMTuiP57W+LnxqaMCH/j7jYH/Rhh4X+ozKnhEp58cWwje23D6eQXngh7K+xY8Py3FNOues69VQ4/viwHyAEx+GHZwMyNyh69gyn0zKnWop59lkYMaL+utbVrVsYnn46/N204HOlgK2/f6xW6YK+VONBWx1au7G4lMWLwxUwpaxfH3/Drob4hwULGle+qip/ety48PuwZk24QqrU+yZOzI4vW5Ydv/de98rKMH7wwfVvO+P444svf/5597PPDuNduoTXMWPCex59NFvu0kvdN21yf/zx0Mifq9Q2i/n979333Tdb9pJLwuvdd9f/vkzD+1lnuS9d6v7GG/WXHzEilG/uVTX1+fTT8uoeh3XrwrbjbCwuOrMtD20lCMpR94+mf3/36dPz533ta+4//3nLfXFpSHb4xS8K53Xr1jrbLvV7V2zYeefwOnp08fdUVITXSZPcq6vdn302/ONTapvl/P4XC4LaWve1a7NlpkzJjh90kPuee2anV6zIvm/atOxlyJkgGDEilP/442y5+fPd77rLvabG/Z573Fetatrf8je+Ebaxww4h4Brjo4/cb7+99PI5c8Jl2HVDNyOzfxQEOUN7DoL33y+cn3H//WF6993dP/ssu3ynnVrnS0RD+x9K/d41NHz0UellDz7oPnBg6eVr1oRt/ulP4R6KmprS9cgEwVVXhdeHHnK/4478MgcemB3fY4/8ZTNmhMu4R40K06ecErZz2GFhetttw+utt4b5q1dn33vuueH17LNDsP3iF+GoPWPNmnC9frl/y425VDWz/z74IDvvvffcZ84MX/577539ORQze3bhz7gpFAQJKfWH+thj4ZRB3R/81Knun3wSxhcuDNe519S4X3llOEx+/fXsL3RDw113hSHpLycNrTdMmODeu3fj3/eFLzRvu2ecUTjvl79s+H0VFe5f/nLztpP7d7bdduG1Z89wqmvWrOyyr30tvA4d6n7ffWF81Khw6qnuUdzIke533hnWPXKk++DBhdt9883Sf/dLl+Yv79UrvCfz9z53bnY9mVCEEBT//Gfh+m65JVumvrBqiIIgIddeG/bwMcdkf2lbwrhxYX1XXln8D6bYf2WzZoVzv0l/WWnQ0JLDpk2Nf89xx4XXn/ykacEJ4b/08eNDO0pNjftPfxradtzdd9wxlMl8aWfek5lf6p+5G24Ir717h38E3347vP/mm7Nl5s5t+veGgiBhW7bkfzm3hLVrw+tNN+X/Mj3wQH65zPxMPa65Jv9Qs9xh//3zz+Vq0JDmodQ/VblHD08+GS4qac527rgj//TwD3/Y9O+MxIIAGAq8CywArimyvBNQGS1/DejT0DrbYxC0hsrK0EdSqas8iqmuDo1pueUywwMPFG+8euyxbJlp08LrFVeEQ/HMOdrcIdMo2dzhwAMb12eSBg1b49C5c9O/IxIJAqACWAj0A7YFZgEH1ilzCTA2Gj8DqGxovQqCxnnssdCvTkM6d3Y/+eTsL1wptbXuv/qV+6uvZqczVq92P/bY7BHHkCHhsrs77nAfMCCcnlq/3v3qq9379XPfbz/33/wmlJ8yJRz2Pv+8+/XXZ+sxcmS2s7nXXgvzxowJbSm5V1vlXv6YGeo2NpYa5s8vnJe5vLLc4dVXk/+S0JCOoamSCoIjgWdypq8Frq1T5hngyGi8A7AcsPrWqyCI18KF4Uu1ubZsKX05XDmWLMm/DLAc8+Zlz9PWNWlS9g9p9OhQbuzYwjp+8on7u++G8fnzQxBt2RIuPTzrrGyoVleHzsDmzMl2hHfffeFKmGnTQsP++PH528yM779/dnzoUPenn3YfNsz9zDNDw+I++4SwfOaZcGVMsS+Dc87Jjg8YkL8stwEyM2SutNHQ/oemqi8ILCxveWZ2OjDU3S+Kps8B/s3dL8spMycqsyiaXhiVWV5nXRcDFwPsvffeX/mwoaepiLRhW7bAtGmF3Ww0ZPly2GGH0EfRvvuGLiE6dw53Jte1Zk3Yxre+lT/fPWx/9erS3WnULV9TE7rTeOut8DCarl1D9xadOoVHvY4eHeq1ZEm4C3a77cKwbFno1uOTT8Kwyy7huRVr14btv/tu6JupY8fwdLuNG2HevHAn9/bbh76ZDjoodO3hDrvvHjqWO+mkcDfxp5+Gut1xR1j/j34U7pyeMiW8/7rr4Oc/D92/dOsW1v0f/xH6MurTJ9xJPWcOfPnLYV8eeWSo/+bN8PzzoVeBTZtg+PDwcKdTTgn9Hw0ZEtbx4x+Hu6wffTT063TIIaFvrQceKNyP69ZBZWXoRmWnncKd3pWVcNNN4bGy22wDjzwSHg179NGhC5E1a2D//UOXHwMGhDvan3yyvJ9bMWY23d2L3r/fLoIg16BBg7yqpe9RFxHZytUXBHH2NbQY2Ctnulc0r2gZM+sA7AyU0cmwiIi0lDiD4HWgv5n1NbNtCY3BE+qUmQD8IBo/Hfi7x3WIIiIiRXWIa8XuXmNmlxEahCuA+919rpndSGi0mAD8Dvh/ZrYAWEkICxERaUWxBQGAu08EJtaZd33O+Ebge3HWQURE6qfnEYiIpJyCQEQk5RQEIiIppyAQEUm52G4oi4uZLQOaemtxd0I3FmmmfaB9ANoHkL590NvdexRb0O6CoDnMrKrUnXVpoX2gfQDaB6B9kEunhkREUk5BICKScmkLgvuSrkAboH2gfQDaB6B98LlUtRGIiEihtB0RiIhIHQoCEZGUS00QmNlQM3vXzBaY2TVJ16clmdn9ZrY0etBPZt6uZjbZzOZHr12j+WZmd0X74U0zOzTnPT+Iys83sx8U21ZbZGZ7mdlUM3vLzOaa2eXR/DTtg85mNs3MZkX74H+i+X3N7LXos1ZGXcJjZp2i6QXR8j4567o2mv+umR2fzCdqOjOrMLM3zOzpaDp1+6DRSj3DcmsaCN1gLwT6AdsCs4ADk65XC36+o4FDgTk5824FronGrwF+GY2fCEwCDDgCeC2avyvwXvTaNRrvmvRnK/Pz7w4cGo3vCMwDDkzZPjCgSzTeEXgt+myPAmdE88cCP4zGLwHGRuNnAJXR+IHR30cnoG/0d1OR9Odr5L64EvgD8HQ0nbp90NghLUcEhwML3P09d98M/BE4JeE6tRh3f4HwPIdcpwAPRuMPAt/NmT/Og1eBXcxsd+B4YLK7r3T3fwGTgaHx17753H2Ju8+IxtcCbwN7kq594O6+LprsGA0OHAuMj+bX3QeZfTMe+KaZWTT/j+6+yd3fBxYQ/n7aBTPrBXwb+G00baRsHzRFWoJgT+DjnOlF0bytWU93XxKNfwr0jMZL7YutYh9Fh/eHEP4jTtU+iE6JzASWEkJsIbDK3WuiIrmf5/PPGi1fDXSjne8D4ClBdeMAAAMrSURBVE7gaqA2mu5G+vZBo6UlCFLNw/HuVn+dsJl1AR4HRrr7mtxladgH7r7F3QcSng9+OLB/wlVqVWb2HWCpu09Pui7tTVqCYDGwV850r2je1uyf0ekOotel0fxS+6Jd7yMz60gIgYfd/Ylodqr2QYa7rwKmAkcSTntlnkSY+3k+/6zR8p2BFbTvffBV4GQz+4Bw+vdY4Nekax80SVqC4HWgf3T1wLaEhqEJCdcpbhOAzFUvPwCezJl/bnTlzBHA6uj0yTPAcWbWNbq65rhoXpsXndf9HfC2u/9vzqI07YMeZrZLNL4dMITQVjIVOD0qVncfZPbN6cDfo6OmCcAZ0RU1fYH+wLTW+RTN4+7Xunsvd+9D+Bv/u7ufTYr2QZMl3VrdWgPhSpF5hPOm/5V0fVr4sz0CLAGqCeczLySc6/wbMB+YAuwalTVgTLQfZgODctZzAaFhbAFwftKfqxGf/2uE0z5vAjOj4cSU7YMBwBvRPpgDXB/N70f4ElsAPAZ0iuZ3jqYXRMv75azrv6J98y5wQtKfrYn7YzDZq4ZSuQ8aM6iLCRGRlEvLqSERESlBQSAiknIKAhGRlFMQiIiknIJARCTlFAQiTWBmI81s+6TrIdISdPmoSBNEd68OcvflSddFpLl0RCDSADPbwcz+EvX1P8fMbgD2AKaa2dSozHFm9oqZzTCzx6J+jzCzD8zsVjObHT0v4ItJfhaRYhQEIg0bCnzi7ge7+5cIPVx+AnzD3b9hZt2B64BvufuhQBWhT/yM1e7+ZeDu6L0ibYqCQKRhs4EhZvZLM/u6u6+us/wIwsNMXo66gf4B0Dtn+SM5r0fGXluRRurQcBGRdHP3edHjLE8EbjKzv9UpYoQH2pxZahUlxkXaBB0RiDTAzPYA1rv7Q8BthMeCriU8FhPgVeCrmfP/UZvCvjmrGJbz+krr1FqkfDoiEGnYl4HbzKyW0MPrDwmneP5qZp9E7QTnAY+YWafoPdcRersF6GpmbwKbgFJHDSKJ0eWjIjHSZabSHujUkIhIyumIQEQk5XREICKScgoCEZGUUxCIiKScgkBEJOUUBCIiKff/AXEZIrPkuXoOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE82U2V2bJ9_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}